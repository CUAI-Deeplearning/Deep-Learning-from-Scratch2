{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션의 구조\n",
    "* 어텐션 : seq2seq를 필요한 정보에만 주목하게 함.\n",
    "    * 이를 통해 seq2seq가 갖고 있는 문제를 해결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq의 문제점\n",
    "* 이전의 seq2seq는 Encoder가 고정 길이의 벡터를 출력  \n",
    "    * 아무리 긴 문장이 있더라도 고정된 길이의 벡터로 반환해야 하기 때문에 한계가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 개선\n",
    "* 시각 별 LSTM 계층의 은닉 상태 벡터를 모두 사용 : 이렇게 되면 입력된 단어 길이와 같은 수의 벡터를 얻을 수 있다. \n",
    "* 여러 딥러닝 프레임워크에서 RNN 계층을 초기화할 때 모든 시각의 은닉 상태 벡터 변환과 마지막 은닉 상태 벡터만 반환 둘 중 하나로 설정할 수 있다.\n",
    "* 각 시각의 은닉 상태에는 해당 시각에 입력된 단어의 정보가 많이 담겨 있을 것이다.\n",
    "* 문장에서 단어의 정보는 주변 정보를 균형 있게 담아야 하기 때문에 양방향 RNN이나 양방향 LSTM을 사용하면 효과적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선 1\n",
    "* **얼라인먼트(alignment)**: 단어(혹은 문구)의 대응 관계를 나타내는 정보, 이전까지는 사람의 수작업으로 만들어졌다. \n",
    "    * 어텐션 : 얼라인먼트 자동으로 수행\n",
    "\n",
    "* 어텐션 : 필요한 정보에만 주목해서 그 정보로부터 시계열 변환을 수행\n",
    "\n",
    "<img src='./images/fig_8-6.png' width=700>\n",
    "\n",
    "* 입력 : Encoder로부터 온 hs와 시각별 LSTM 계층의 은닉 상태이다. \n",
    "    * 필요한 정보만 골라 Affine 계층으로 출력한다.\n",
    "    * Encoder의 마지막 은닉 상태 벡터 : Decoder의 첫번째 LSTM 계층로 전달된다.\n",
    "\n",
    "* Decoder에서 출력을 할 때 필요한 정보를 hs에서 선택하는 과정이다. \n",
    "    * 하지만 선택하는 작업은 미분할 수 없다는 문제 발생\n",
    "    * 이 문제를 **전체를 선택한 후 각 단어의 가중치를 별도로 계산하는 문제로 치환**하면 미분 가능\n",
    "\n",
    "<img src=\"./images/fig_8-8.png\" width=600>\n",
    "\n",
    "* 각 단어의 중요도를 나타내는 가중치 a(확률분포와 유사)를 구한 후 단어 벡터 hs와 가중합을 계산해서 맥락 벡터 c를 구한다. \n",
    "    * '나'에 대응하는 가중치가 0.8이었으니 맥락 벡터에는 '나' 벡터의 성분이 많이 포함된 걸로 해석 가능\n",
    "    * 맥락 벡터 c : 현 시각의 변환을 수행하는데 필요한 정보가 담겨야 한다.\n",
    "    * 결국 이렇게 학습하도록하는 것이 궁극적인 목표이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 처리 : 가중합 구현\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "# ar = a.reshape(N, T, 1) # 브로드캐스트를 사용하는 경우\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선 2\n",
    "* 가중치 a도 데이터로부터 자동으로 학습할 수 있도록 해야 한다.\n",
    "* Decoder의 LSTM 계층의 은닉 상태 벡터(h)와 Encoder의 마지막 시각의 hs 사이의 유사도는 벡터의 내적으로 구할 수 있다. \n",
    "    * 내적은 두 벡터가 얼마나 같은 방향을 향하고 있는지를 나타낸다. 내적을 하면 정규화하기 전의 점수 s를 구할 수 있다. \n",
    "* s는 일반적으로 소프트맥스 함수를 거쳐 정규화해 가중치 a를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선 3\n",
    "\n",
    "<img src=\"./images/fig_8-16.png\" width=600>\n",
    "\n",
    "현재까지 Weight Sum, Attention Weight 게층이라고 하고, 이 둘을 합쳐서 Attention 계층이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-18.png\" width=700>\n",
    "\n",
    "* Affine 계층에 맥락 벡터와 은닉 상태 벡터가 입력\n",
    "* 다수의 Attention 계층을 모아 Time Attention 계층 구현 가능\n",
    "\n",
    "<img src=\"./images/fig_8-20.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션을 갖춘 seq2seq 구현\n",
    "### Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from attention_layer import TimeAttention\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 평가\n",
    "### 날짜 형식 변환 문제  \n",
    "\n",
    "* 번역용 데이터셋 : WMT\n",
    "    * 너무 커서 날짜 형식 변경 문제로 대체\n",
    "    \n",
    "* 사람이 쓴 날짜 데이터(형식에 맞추지 않음)를 표준 형식(YYYY-MM-DD)으로 변환\n",
    "\n",
    "    * 사람이 쓴 날짜 데이터의 형식은 매우 많으므로 수작업으로 해당 패턴을 찾아서 변환 규칙을 만드는 것은 매우 번거롭다.  \n",
    "    * 입력과 출력의 대응 관계를 쉽게 알 수 있다.\n",
    "\n",
    "<img src=\"./images/fig_8-23.png\" width=500>\n",
    "\n",
    "* 입력과 출력의 구분 문자는 \\_, 출력의 끝은 출력의 길이가 모두 동일해서 따로 두지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import SGD\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 6[s] | 손실 4.06\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 12[s] | 손실 4.03\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 19[s] | 손실 4.00\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 25[s] | 손실 3.97\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 31[s] | 손실 3.93\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 37[s] | 손실 3.90\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 43[s] | 손실 3.87\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 49[s] | 손실 3.84\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 55[s] | 손실 3.80\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 61[s] | 손실 3.77\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 67[s] | 손실 3.73\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 73[s] | 손실 3.69\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 79[s] | 손실 3.65\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 85[s] | 손실 3.61\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 91[s] | 손실 3.56\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 97[s] | 손실 3.51\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 103[s] | 손실 3.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m ----------\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 3.40\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 6[s] | 손실 3.36\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 12[s] | 손실 3.28\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 18[s] | 손실 3.20\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 25[s] | 손실 3.11\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 31[s] | 손실 3.02\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 37[s] | 손실 2.92\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 43[s] | 손실 2.82\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 49[s] | 손실 2.73\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 55[s] | 손실 2.65\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 61[s] | 손실 2.59\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 67[s] | 손실 2.53\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 73[s] | 손실 2.50\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 79[s] | 손실 2.46\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 85[s] | 손실 2.42\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 91[s] | 손실 2.39\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 97[s] | 손실 2.38\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 102[s] | 손실 2.34\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1---------\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 0---------\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1---------\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 0---------\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 0---------\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1---------\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1---------\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1---------\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1---------\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 0---------\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 2.34\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 6[s] | 손실 2.32\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 12[s] | 손실 2.30\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 18[s] | 손실 2.28\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 23[s] | 손실 2.27\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 29[s] | 손실 2.26\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 35[s] | 손실 2.24\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 41[s] | 손실 2.23\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 48[s] | 손실 2.22\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 54[s] | 손실 2.21\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 60[s] | 손실 2.20\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 66[s] | 손실 2.19\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 72[s] | 손실 2.19\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 78[s] | 손실 2.18\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 84[s] | 손실 2.17\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 90[s] | 손실 2.16\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 96[s] | 손실 2.16\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 102[s] | 손실 2.15\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 10-0------\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 11--------\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 10-0------\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 11--------\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 11--------\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 11--------\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 10-0------\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 10-0------\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 10-0------\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 11--------\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 2.14\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 6[s] | 손실 2.14\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 12[s] | 손실 2.13\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 19[s] | 손실 2.14\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 25[s] | 손실 2.12\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 31[s] | 손실 2.12\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 37[s] | 손실 2.12\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 43[s] | 손실 2.11\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 49[s] | 손실 2.10\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 56[s] | 손실 2.10\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 62[s] | 손실 2.10\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 68[s] | 손실 2.10\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 74[s] | 손실 2.09\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 80[s] | 손실 2.08\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 86[s] | 손실 2.08\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 92[s] | 손실 2.08\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 98[s] | 손실 2.07\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 104[s] | 손실 2.07\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 110-0--0--\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 11-0------\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 11-0--0---\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 11-0------\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 11-0------\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 11-0--0---\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 110-0--0--\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 110-0--0--\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 110-0--0--\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 11-0------\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 2.07\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 6[s] | 손실 2.06\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 12[s] | 손실 2.06\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 18[s] | 손실 2.06\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 24[s] | 손실 2.05\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 30[s] | 손실 2.05\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 36[s] | 손실 2.04\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 42[s] | 손실 2.04\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 48[s] | 손실 2.04\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 54[s] | 손실 2.03\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 61[s] | 손실 2.03\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 67[s] | 손실 2.02\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 73[s] | 손실 2.02\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 79[s] | 손실 2.02\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 85[s] | 손실 2.02\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 91[s] | 손실 2.01\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 97[s] | 손실 2.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 103[s] | 손실 2.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 110-0-0-0-\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 11-0-0-0-0\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 110-0-0-0-\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 11-0-0-0-0\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 11-0-0-0-0\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 11-0-0-0-0\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 110-0-0-0-\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 110-0-0-0-\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 110-0-0-0-\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 11-0-0-0-0\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 2.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 6[s] | 손실 2.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 12[s] | 손실 1.99\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 18[s] | 손실 1.99\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 24[s] | 손실 1.98\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 31[s] | 손실 1.98\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 37[s] | 손실 1.98\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 43[s] | 손실 1.97\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 50[s] | 손실 1.97\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 55[s] | 손실 1.96\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 62[s] | 손실 1.96\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 68[s] | 손실 1.95\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 73[s] | 손실 1.95\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 80[s] | 손실 1.94\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 86[s] | 손실 1.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 6 |  반복 301 / 351 | 시간 92[s] | 손실 1.93\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 98[s] | 손실 1.93\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 104[s] | 손실 1.92\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 19-0-0-0-0\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 6[s] | 손실 1.91\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 11[s] | 손실 1.91\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 17[s] | 손실 1.91\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 23[s] | 손실 1.90\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 29[s] | 손실 1.90\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 35[s] | 손실 1.89\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 40[s] | 손실 1.88\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 46[s] | 손실 1.88\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 52[s] | 손실 1.88\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 58[s] | 손실 1.87\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 64[s] | 손실 1.86\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 70[s] | 손실 1.85\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 75[s] | 손실 1.85\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 81[s] | 손실 1.85\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 87[s] | 손실 1.84\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 93[s] | 손실 1.83\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 99[s] | 손실 1.83\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 6[s] | 손실 1.82\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 11[s] | 손실 1.82\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 17[s] | 손실 1.81\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 23[s] | 손실 1.80\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 29[s] | 손실 1.80\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 34[s] | 손실 1.79\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 40[s] | 손실 1.78\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 46[s] | 손실 1.78\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 52[s] | 손실 1.77\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 58[s] | 손실 1.77\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 63[s] | 손실 1.76\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 69[s] | 손실 1.76\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 75[s] | 손실 1.76\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 81[s] | 손실 1.75\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 87[s] | 손실 1.74\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 92[s] | 손실 1.74\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 98[s] | 손실 1.73\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 6[s] | 손실 1.72\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 11[s] | 손실 1.71\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 17[s] | 손실 1.71\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 23[s] | 손실 1.71\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 29[s] | 손실 1.70\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 34[s] | 손실 1.69\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 40[s] | 손실 1.69\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 46[s] | 손실 1.68\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 52[s] | 손실 1.68\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 58[s] | 손실 1.67\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 63[s] | 손실 1.67\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 69[s] | 손실 1.66\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 75[s] | 손실 1.66\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 81[s] | 손실 1.66\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 87[s] | 손실 1.65\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 92[s] | 손실 1.64\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 98[s] | 손실 1.64\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 6[s] | 손실 1.63\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 11[s] | 손실 1.62\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 17[s] | 손실 1.62\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 23[s] | 손실 1.62\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 29[s] | 손실 1.61\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 35[s] | 손실 1.61\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 41[s] | 손실 1.60\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 47[s] | 손실 1.60\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 53[s] | 손실 1.59\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 58[s] | 손실 1.59\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 64[s] | 손실 1.58\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 70[s] | 손실 1.59\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 76[s] | 손실 1.58\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 82[s] | 손실 1.57\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 87[s] | 손실 1.56\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 93[s] | 손실 1.57\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 99[s] | 손실 1.56\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 199-0-0-01\n",
      "---\n",
      "val acc 0.000%\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "        \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "    \n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 6[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 12[s] | 손실 1.87\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 18[s] | 손실 1.66\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 24[s] | 손실 1.31\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 30[s] | 손실 1.17\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 36[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 42[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 48[s] | 손실 1.05\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 55[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 61[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 67[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 73[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 79[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 85[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 91[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 97[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 103[s] | 손실 0.99\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1992-03-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 6[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 12[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 18[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 25[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 31[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 37[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 43[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 49[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 55[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 61[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 67[s] | 손실 0.96\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 73[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 79[s] | 손실 0.91\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 85[s] | 손실 0.87\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 91[s] | 손실 0.81\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 97[s] | 손실 0.76\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 103[s] | 손실 0.68\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1994-08-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2001-11-14\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 2003-09-28\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 2011-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1992-10-08\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 2003-08-28\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-04-04\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1993-08-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2011-11-24\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 6[s] | 손실 0.54\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 11[s] | 손실 0.43\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 17[s] | 손실 0.33\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 23[s] | 손실 0.24\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 29[s] | 손실 0.17\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 34[s] | 손실 0.11\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 40[s] | 손실 0.08\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 46[s] | 손실 0.06\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 52[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 58[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 63[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 69[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 75[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 81[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 87[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 92[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 98[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 6[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 12[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 18[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 24[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 30[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 36[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 42[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 48[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 61[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 101[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 107[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 6[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 19[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 25[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 38[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 44[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 50[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 69[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 100[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 107[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 6[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 25[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 37[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 43[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 61[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 85[s] | 손실 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 6 |  반복 301 / 351 | 시간 91[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 102[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 6[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 24[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 35[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 65[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 77[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 100[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 7[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 13[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 19[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 25[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 38[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 44[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 50[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 99[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 105[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 6[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 25[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 37[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 43[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 61[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 84[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 102[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 6[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 35[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 65[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 70[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 76[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 100[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 0.000%\n"
     ]
    }
   ],
   "source": [
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list1 = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "        \n",
    "    acc1 = float(correct_num) / len(x_test)\n",
    "    acc_list1.append(acc1)\n",
    "    print('val acc %.3f%%' % (acc1 * 100))\n",
    "    \n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='v')\n",
    "plt.plot(x, acc_list1, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-26.png\" width=600>\n",
    "\n",
    "* 기본 seq2seq : 낮은 정확도\n",
    "* peeky와 어텐션 : 비슷한 정확도, 어텐션이 학습 속도우세\n",
    "* 현실의 시계열은 훨씬 복잡 : 어텐션 추천\n",
    "* 어텐션 쓰더라도 SGD쓰면 최악의 결과가 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션 시각화\n",
    "* Time Attention 계층의 인스턴스 변수 attention_weight에 각 시각의 어텐션 가중치를 이용\n",
    "    * 입력 문장과 출력 문장의 단어 대응 관계를 2차원 맵으로 그릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(2019)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션에 관한 남은 이야기\n",
    "### 양방향 RNN\n",
    "* 양방향 LSTM에 역방향도 처리하는 LSTM 계층을 추가\n",
    "    * 각 시각마다 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉 상태로 처리\n",
    "    * 서로 합하거나 평균하는 방법도 있다.\n",
    "\n",
    "* 이점 : 은닉 상태 벡터에 좌, 우 양쪽 방향의 균형 잡힌 정보를 인코딩 가능\n",
    "* 구현 : 단순히 입력 단어를 반대로 나열해서 입력하는 LSTM 계층을 하나 더 만든 후, 정방향으로 처리하는 계층의 출력과 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션 계층 사용 방법\n",
    "<img src=\"./images/fig_8-32.png\" width=700>\n",
    "\n",
    "* Attention 계층의 출력이 다음 시각의 LSTM 계층에 입력되도록 연결\n",
    "* 직접 구현한 경우는 Affine 계층에서 맥락 벡터를 이용한 반면 이 경우에는 LSTM 계층에서 맥락 벡터를 이용\n",
    "\n",
    "* 구현 측면\n",
    "    * LSTM과 Affine 계층 사이에 Attention 계층을 삽입하는 것이 쉽다.\n",
    "    * 최종 정확도 : 큰 차이가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 심층화와 skip 연결\n",
    "<img src=\"./images/fig_8-33.png\" width=700>\n",
    "\n",
    "* 일반적으로 Encoder과 Decoder의 LSTM 계층을 동일한 수로 구성\n",
    "* 여러 방식으로 심층화 가능\n",
    "\n",
    "<img src=\"./images/fig_8-34.png\" width=300>\n",
    "\n",
    "* **skip 연결** : 층을 깊게 할 때 기울기가 잘 흐를 수 있도록 하는 방법\n",
    "    * 계층을 건너뛰는 연결\n",
    "    * LSTM의 출력과 LSTM을 통과하지 않은 출력의 잔차의 원소별 덧셈을 해준다.\n",
    "    * 덧셈은 역전파 시 기울기를 그대로 흘려보낼 수 있게 해준다.\n",
    "\n",
    "* 층을 깊게 하면 여전히 오버피팅 문제가 발생 가능\n",
    "    * 일반화 성능을 유지해주기 위해서 드롭아웃과 가중치 공유 등의 기술을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 응용\n",
    "### 구글 신경망 기계 번역(GNMT)\n",
    "<img src=\"./images/fig_8-35.png\" width=700>\n",
    "\n",
    "### 트렌스포머\n",
    "* 트랜스포머 : RNN 대신 어텐션을 사용\n",
    "<img src=\"./images/fig_8-37.png\" width=600>\n",
    "<img src=\"./images/fig_8-38.png\" width=600>\n",
    "\n",
    "### 뉴럴튜닝머신(NMT)\n",
    "<img src=\"./images/fig_8-41.png\" width=700>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
